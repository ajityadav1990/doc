Kubernetes :- it provide high availability and centralise management  capability  over the contener technology .
Adavantag : 
1)  centerlise management 
2) maintene high_availabilty ( node lable /contner lable )
3) scalability ( manual / automatically)
4 ) project lable isolation
5 ) user level role access.
6 ) CPU /memory level quota apply
7 ) POD scheduling
8 ) rolingupgrade

1) Kubernetes master

2) Kubernetes Node /minion /worker
1 ) Kubernetes master 

we have  4 service running on kubernetes master 3 for NODE 

1) kube-api service : it handel all communication channel into kubernetes  
   example : service to service / user authentication / authrization /role
2) kube-etcd  : is yaml  based database. it container kubernetes data in persistent form we have always take backup this
3) kube-controller : it handel communication with every kube-node using api with kubelet
4) kube-scheduler  :  it determin best fit node for container.
 
user - command-api --etcd -- controller -- kubelet- docker- container

2 ) Kubernetes Node /minion /worker

1) Docker
2) Kubelet
3) kubeproxy

Quick installatation using kubeadm  utility/control plain installation

Hardway installation ( every component manual method )
lab setup 
master : kubeadm/docker
command : kubeadm init

node1 : kubeadm/docker 
node2 : kubeadm/docker

kubeadm is installation utility
https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
copy peston desktop

cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
EOF
 
cat /etc/yum.repo.d

yum install docker kubeadm -y
systemctl start kubelet docker
systemctl enable kubelet docker

kubeadm init 
kubeadm init --ignore-preflight-errors=all
mkdir .kube
cp -i /etc/kubernetes/admin.conf $home/.kube/config    ( copy it to master user profile) 

mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

for token 
kubeadm token list

for CRt file
[root@ip-172-31-43-158 ec2-user]# openssl x509 -pubkey \
> -in /etc/kubernetes/pki/ca.crt | openssl rsa \
> -pubin -outfrom der 2>/dev/null | openssl dgst \
> -sha256 -hex | sed 's/^.* //'

three methord work on kubernet

1) Kubctl
2) GUI
3) File methord (yaml)

kubeadm token list     ( show token list )
kubectl get pod
kubectl get namespace

kubectl get pod -n kube-system


if we setup with kubeadm it always setup internal DNS ( with A record )
 
SetUp NODE : -
Node 1 

cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
EOF

yum install docker kubeadm -y
systemctl start kubelet
systemctl start kubelet docker
systemctl enable kubelet docker

kubeadm join 172.31.30.228:6443 --token yuvbxi.yc6mrndw22p6rumd \
    --discovery-token-ca-cert-hash sha256:42833a1689d08c84d3de10712067d2e6da7e43fe35a2cfd24602b8d13bdcda70 
   
kubectl get node     ( check how many node connect to the master )
kubectl describe node node_name   ( print log spacic node )

POD Network setup   with SDN technolgy ( depolying network switch pod comunition )

 weave network create in kubernetes (google it )
 
 copy pest 
$ kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"

ip a s weave   ( check weave network setup on node )
kubectl get pod -n kube-system   ( check on master it depoly new pod for weave network )
heare we found DNS auto maticaly up because network depoly.
kubectl run --generator=run-pod/v1 test --image=nginx   (it is in flat deployment ( drow back  kubernet not moniter this if you delete pod new pod will not scal up automaticle)
kubectl get pod
 kubectl create namespace prod
    kubectl create namespace dev
    kubectl create namespace stage
    kubectl run test --image=nginex:latest -n prod
    kubectl run test --image=nginex:latest
    kubectl get deployment
    kubectl get pod
    kubectl get pod -o wide
    kubectl delete deployment test
    kubectl run test --image=nginex:latest --replicas=2
    kubectl get pod
    kubectl delete pod test-557686b896-qfj4g 
    kubectl get pod
    kubectl scale --replicas=3 deployment test
    kubectl get pod
    kubectl describe deployment test
    kubectl edit deployment test
    kubectl get pod
    kubectl get namespace
    kubectl get pod
    kubectl delete deployment test
    kubectl run test  --image=nginex:latest -o yaml --dry-run > is.yml
    kubectl run test  --image=nginex:latest-o json --dry-run > is.json
    kubectl create -f is.yml 
    kubectl get deployment
    
        rm -rf is.yml
    kubectl get deployment test -o yaml > is.yml
    vim is.yml
    kubectl apply -f is.yml

service : it provide single endpoint to access deployment pod
Kind of act as balancer ip
Qproxy is use for balancer 
we will setup single ip for multple pod for blancer
Service :
Internal use :
External use :

kubectl expose deployment test  --port=80
kubectl get service
kubectl exec -it test-f5666fbf9-2mc2g /bin/bash
echo pod1 > /var/www/html/index.html
exit
curl 10.107.1.61
kubectl describe service test
kubectl scale --repliacas=3 deployment test
kubectl describe service test
kubectl get service test -o yaml > is.yml
kubectl delete service test
vim is.yml
kubectl create -f is.yml
kubectl get service 
kubectl delete service test
kubectl expose deployment test --port=80 --type=NodePort
kubectl get service 

kubectl run test --image=nginx --replicas=2
kubectl exec -it test-db5495c67-5mgg8  /bin/bash
kubectl expose deployment test --port=80
kubectl get service 
External IP ...

kubectl delete service test
kubectl expose deployment test --type=NodePort --port=80
kubectl get service 














 


 
















 

Promathises 
